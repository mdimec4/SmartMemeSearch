we are using: https://huggingface.co/Xenova/clip-vit-base-patch32/tree/main/onnx

1. Use this model repo: Xenova / clip-vit-base-patch32

This repo has working ONNX weights for both image and text, and is actively maintained. 
Hugging Face

Do this:

Open your browser and go to HuggingFace.

In the search box, type:
Xenova clip-vit-base-patch32

Open the model page called:
“Xenova/clip-vit-base-patch32”

Go to the “Files and versions” tab, then click into the onnx/ folder.
You should see files like:

vision_model.onnx

text_model.onnx

plus a bunch of quantized variants (*_fp16.onnx, *_quantized.onnx, etc.) 
Hugging Face

Click vision_model.onnx → click Download (button on the right or in the file view).

Click text_model.onnx → Download.

No need for special direct links, just use the HF UI.

2. Place and rename files in your project

In your project folder:

SmartMemeSearch/
  Assets/
    clip_image.onnx   ← rename of vision_model.onnx
    clip_text.onnx    ← rename of text_model.onnx
